---
layout: post
title: "Kaggle Kore2022 대회 후기"

category: study
sitemap: false
hide_last_modified: true
---
이번 포스팅에서는 Kaggle에서 열린 Kore2022라는 시뮬레이션 대회를 되돌아보려 한다. 주제에 맞는 훈련용 데이터를 제공하고 테스트 데이터에 대해 정확도를 평가하는 여타 지도학습 대회와 다르게 시뮬레이션 대회는 게임과 같은 상호작용할 수 있는 환경을 제공한다. 매 턴마다 주어진 상황에 적절한 액션을 환경에 인풋으로 넣어주면 다음 턴에 그 액션이 실행되면서 상황이 변하고 다시 이에 맞는 액션을 넣어주는 것을 반복하는 것이다. 마치 알파고처럼 상황에 적절한 액션만을 취하는 방법을 훈련하는 것이 목적이 시뮬레이션 대회라 할 수 있다. 물론 우리에게는 바둑, 벽돌깨기, 스타크래프트, 도타 등의 게임 환경이 잘 알려져 있지만 실제로는 로봇이나 모션 관련 환경도 굉장히 많다. 나는 게임 AI 연구실에 진학하면서 강화학습 공부를 시작했는데 6월에 이 대회를 처음 참가했을 때 기초를 대부분 뗀 터라 논문뿐만이 아니라 구현체들도 보고, 실제 환경에서 응용할 때 어떻게 해야하는지 알고싶어 대회에 참여했다.

결론적으로 말하면 전체 469팀에서 172등, 상위 36.6%로 수상에 실패했다. 심지어 강화학습 알고리즘이 동작하지 않아 룰베이스 알고리즘으로 제출했다. 강화학습 프레임워크로는 stable-baseline3를 사용했고 환경 래핑함수는 직접 구현했다. 성능이 문제가아니라 학습 자체를 실패해 결국 강화학습은 포기했다. 밤낮주말 가릴거 없이 공을 많이 들인 대회였는데 기본적인 성능도 나오지 않아 크게 낙담했었다. 하지만 막상 수상 결과가 나오고보니 오프라인 데이터로 Imitation learning을 한 13등, 34등을 제외하고 순위권들은 모두 룰베이스 알고리즘이었으며 강화학습으로 학습에 성공한 사람은 Discussion에 단 한 명도 나타나지 않았다. 나만 학습에 실패한 것이 아니라 환경자체가 강화학습으로 푸는게 어려웠던 것이다. 강화학습이 불안정하다는 얘기는 들어왔으나 이렇게 간단한 환경 조차 학습에 실패할 수 있다는 사실은 처음 알게되었다. 이후 게임에 AI를 적용하는 연구를 하려 대학원에 진학했던 나는 강화학습에 산재한 문제점을 연구하는 것에 더 관심이 생기게 되었다. 


<p align="center">
<img src = "..\assets\img\Kaggle\Kore2022_general.png"
 width="70%" />
</p>

Kore2022는 가로 21칸, 세로 21칸 길이의 보드위에서 자원을 채취하고 비행기를 생산하여 자원을 더 많이 먹거나 상대방을 제거하여 승리하는 턴제 게임이다. 게임은 400턴 동안 계속되며 조선소 하나와 코어라 불리는 자원 500개로 시작한다. 사진에서 라운드 밸브처럼 생긴 심볼이 조선소고 반짝거리는 심볼들이 코어다. 각 조선소에서는 함대를 발사하여 코어를 채취할 수 있다. 사진속 삼각형 모양 심볼들이 함대인데 좌측 위에 보면 함대에 비행기가 얼마나 속해있는지 써져있다. 함대에 비행기가 많이 속해있을수록 한번에 더 많은 자원을 채취할 수 있다. 또 상대방 함대나 조선소와 부딪혔을 때 비행기가 많은 쪽이 상대방의 함대를 파괴하거나 조선소를 탈취하게 되며 들고있던 자원또한 뺏을 수 있다.
이 함대들은 한 턴에 한 칸씩 움직일 수 있는데 여기 가장 중요한 포인트가 있다. 바로 함대들은 발사할 때 미리 계획한 Flight plan에 따라 움직인다는 것이며 Flight plan은 도중에 바꿀 수 없다는 것이다.

한 턴에 한 조선소당 둘 중 하나의 액션을 취할 수 있다. 첫째는 비행기 건조다. 10 코어를 소모해서 비행기 하나를 만들 수 있으며 그 조선소가 오랫동안 생존할수록 한 번에 더 많은 비행기를 생산할 수 있다. 얼마나 비행기가 조선소에 있는지는 좌측 위에 숫자로 표시된다. 둘째는 launch다. 가지고 있는 비행기 중 x대를 함대를 만들어 출격시킨다. 함대는 주어진 Flight plan에 따라 움직이거나 새로운 조선소로 변신할 수 있다. Flight plan은 방향 'N', 'E', 'W', 'S', 숫자 1~99, 그리고 변환 'C'로 조합하는 문자열로 이루어진다.

<p align="center">
<img src = "..\assets\img\Kaggle\Flight_Plan.png"
 width="70%" />
</p>

위에 표에서 보다시피 "E10N3S3W" 의
모델은 DQN, PPO를 사용했다.

왜 이런 결과가 나타나게 되었는지 알아보자. 1등부터 15등까지 휴리스틱, 16등이 Immitation learning.

Kore2022라는 게임은 지금까지 OpenAI나 Deepmind가 정복했던 바둑, 스타, 도타와는 다름.
액션스페이스가 굉장히 크다. 룰은 이렇게된다. sparse reward다. reward shaping이 굉장히 까다롭다. 이런식으로 구성하면 delayed reward가 있으며 이 delayed 되는 step은 가변적이다. 또한 Credit Assignment Problem 이 굉장히 직접적으로 드러난다.
CAP라고 생각한 근거

부족했던 점

멀티에이전트 문제


<!--ㅇㅇ-->

캐글 시뮬레이션 대회

룰

강화학습 적용
PPO

그래프



