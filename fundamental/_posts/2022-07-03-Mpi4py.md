---
layout: post
title: "[CS] 병렬 프로그래밍(2. mpi4py)"
sitemap: false
hide_last_modified: true
---
기초적인 MPI4py 용례에 대해 서술하겠습니다. 이 포스트는 [원본](http://education.molssi.org/parallel-programming/03-distributed-examples-mpi4py/index.html)을 번역한 내용이 주를 이룹니다.


# Mpi4py Tutorial
먼저 리눅스 기준으로 다음 명령어를 통해 openmpi 를 설치해줍니다.
```
sudo apt-get install libopenmpi-dev
```

openmpi가 설치되면 mpirun/mpiexec 명령어를 사용할 수있습니다.  
둘은 비슷한 역할을 하지만 mpiexec이 MPI표준으로 정의됐기 때문에 mpiexec을 쓰길 권장합니다.  
다음 코드를 실행하면서 본격적으로 mpi를 배워봅시다.
mpi_tutorial.py
```
if __name__ == "__main__":
    print("Hello World")
```
```
$ mpiexec -n 4 python mpi_tutorial.py
Hello World
Hello World
Hello World
Hello World
```

-n 은 실행할 프로세스 수를 정의하는 옵션입니다. cpu코어 수를 넘게되면 오류를 출력합니다.
mpi_tutorial2.py
```
from mpi4py import MPI

if __name__ == "__main__":
    world_comm = MPI.COMM_WORLD # Communicator 집합 선언
    world_size = world_comm.Get_size() # Communicator 에 있는 core 수
    my_rank = world_comm.Get_rank() # core 번호

    print("World Size: " + str(world_size) + "  " + "Rank : " + str(my_rank))
```
```
$ mpiexec -n 6 python mpi_tutorial2.py
World Size: 6  Rank : 0
World Size: 6  Rank : 4
World Size: 6  Rank : 5
World Size: 6  Rank : 1
World Size: 6  Rank : 3
World Size: 6  Rank : 2
```
출력순서는 rank 순서대로가 아닐 수 있습니다. 먼저 완료되는 프로세스 순으로 출력되기 때문입니다.
순차프로그래밍으로 작성했을 때 얼마나 시간이 걸리는지 확인해봅시다.
1. 천만의 크기를 가지는 1 배열을 만드는 시간
2. 천만의 크기를 가지는 0 배열에 1~N 의 값을 집어넣는 시간
3. 1 과 2를 더해서 1 배열에 할당하는 시간
4. 1 배열을 평균내는 시간
```
import numpy as np
from mpi4py import MPI

if __name__ == "__main__":
    world_comm = MPI.COMM_WORLD
    world_size = world_comm.Get_size()
    my_rank = world_comm.Get_rank()

    N = 10_000_000

    #initialize a
    start_time = MPI.Wtime() 
    a = np.ones(N) # N 사이즈의 1 배열 할당
    end_time = MPI.Wtime()
    if my_rank == 0: #루트노드는 걸린시간을 출력
        print("Initialize a time: " + str(end_time-start_time))
    
    #initialize b
    start_time = MPI.Wtime()
    b = np.zeros( N )
    for i in range( N ):
        b[i] = 1.0 + i # 1 ~ N 배열 할당
    end_time = MPI.Wtime()
    if my_rank == 0: #루트노드는 걸린시간을 출력
        print("Initialize b time: " + str(end_time-start_time))
    
    #add the two arrays
    start_time = MPI.Wtime()
    for i in range(N):
        a[i] = a[i] + b[i] # a와 b를 더해서 a에 할당
    end_time = MPI.Wtime()
    if my_rank == 0:
        print("Add arrays time: " + str(end_time-start_time))

    start_time = MPI.Wtime()
    sum = 0.0
    for i in range( N ):
        sum += a[i] # 평균
    average  = sum / N
    end_time = MPI.Wtime()
    if my_rank == 0:
        print("Average result time: " + str(end_time-start_time))
        print("Average: " + str(average))
```
```
$ mpiexec -n 6 python mpi_tutorial3.py
Initialize a time: 0.037749559
Initialize a time: 1.607409431
Add arrays time: 5.114151672
Average result time: 3.04743169
Average: 5000001.5
```
# Point-to-Point Communication
위 프로그램을 다수의 코어가 점대점 통신을 사용해서 푸는 프로그램을 설계해 봅시다. 
```
import numpy as np
from mpi4py import MPI

if __name__ == "__main__":
    world_comm = MPI.COMM_WORLD
    world_size = world_comm.Get_size()
    my_rank = world_comm.Get_rank()

    N = 10_000_000

    workloads = [N // world_size for i in range(world_size)] #process 별로 N을 분배
    for i in range(N % world_size):
        workloads[i] += 1

    my_start = 0
    for i in range(my_rank):
        my_start += workloads[i] # 어디부터 어디까지가 업무량인지 할당
    my_end = my_start + workloads[my_rank]

    #initialize a
    start_time = MPI.Wtime() 
    a = np.ones(workloads[my_rank]) # 각 업무량 사이즈의 1 배열 할당
    end_time = MPI.Wtime()
    if my_rank == 0: # 루트노드는 걸린시간을 출력
        print("Initialize a time: " + str(end_time-start_time))

    #initialize b
    start_time = MPI.Wtime()
    b = np.zeros(workloads[my_rank])
    for i in range(workloads[my_rank]):
        b[i] = my_start + i # my_start ~ my_end 배열 할당
    end_time = MPI.Wtime()
    if my_rank == 0: #루트노드는 걸린시간을 출력
        print("Initialize b time: " + str(end_time-start_time))

    #add the two arrays
    start_time = MPI.Wtime()
    for i in range(workloads[my_rank]):
        a[i] = a[i] + b[i] # a와 b를 더해서 a에 할당
    end_time = MPI.Wtime()
    if my_rank == 0:
        print("Add arrays time: " + str(end_time-start_time))

    start_time = MPI.Wtime()
    sum = 0.0
    for i in range(workloads[my_rank]):
        sum += a[i] # 각 코어마다 가지고 있는 값들을 모두 더한다.
    end_time = MPI.Wtime()

    if my_rank == 0: 
        world_sum = sum 
        for i in range(1, world_size):
            sum_np = np.empty(1) #mpi4py 는 numpy array를 이용할 때 가장 빠르다.
            world_comm.Recv([sum_np, MPI.DOUBLE], source=i, tag=77) #i번째 노드들로부터 값이 올 때까지 프로세스를 blocking한다.
            world_sum += sum_np[0] # sum 값을 모두 더한다.
        average  = world_sum / N # 평균을 낸다.
        print("Average result time: " + str(end_time-start_time))
        print("Average: " + str(average))
    else:
        sum_np = np.array([sum]) # np.empty(1) 에 맞는 datatype으로 만들어준다.
        world_comm.Send([sum_np, MPI.DOUBLE], dest=0, tag=77) #destination을 0으로 지정하여 전송한다.
```


참고자료 : http://education.molssi.org/parallel-programming/03-distributed-examples-mpi4py/index.html