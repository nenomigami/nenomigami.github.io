---
layout: post
title: "[RL] Data-Efficient HRL(HIRO) 리뷰"

category: study
sitemap: false
hide_last_modified: true
---

강화학습은 고차원의 상태와 행동 공간을 가지는 경우 잘 동작하지 않습니다. Hierarchical reinforcement learning(HRL) 은 사람의 Hierarchical Reasoning 에서 모티브를 얻어 상태 공간과 행동 공간을 줄이기 위한 접근법입니다. 

Hierarchical RL에는 여러 갈래가 있지만, 이 포스팅에서는 높은 수준의 policy에서 아래로 목표를 부여하는 Feudal learning 계열 방법론 중 하나인 HIRO([Nachum et al., 2018](https://arxiv.org/abs/1805.08296))에 대해 포스팅 하겠습니다.  
  
<br>  

## Hirarchical RL 이란
---

<p align="center">
<img src = "https://user-images.githubusercontent.com/23326843/168076314-c0d740ba-7004-4abd-ae8b-a3e2c4922c9e.png"
 width="60%" />
</p>

위 그림은 Atari 게임 중 몬테주마의 복수라는 게임입니다. Feudal learning 계열 방법론중 HIRO([Nachum et al., 2018](https://arxiv.org/abs/1805.08296)) 보다 앞서 게재되었던  FuN([Vezhnevets et al., 2017](https://arxiv.org/abs/1805.08296)) 에서 experiment 로 제시한 바있습니다. 유저가 다음 스테이지로 넘어가기 위해서는 열쇠를 먼저 찾고, 문으로 가야합니다. 일반적인 강화학습의 exploration으로는 이 규칙을 찾기 쉽지 않아 challenge 로 한동안 남아있었는데요.

Hierarchical RL의 접근법을 사용하여 아랫 단계에서는 특정 목표로 움직이는 방법을 학습하고, 윗 단계에서는 어디를 목표로 설정해야하는지를 학습하는 방법으로 문제를 풀었습니다.

HRL은 위처럼 큰 문제를 작고 간단한 task 여러개로 쪼개어서 sample efficiency를 높이고, 학습 속도를 증가시키는 방법입니다.

예를 하나 더 들어보겠습니다. 우리가 마트에 가서 식재료를 사온다고하면 다음과 같이 진행할 겁니다.
- Top level : 마트 가기, 식재료 사기, 돌아오기
- "마트 가기" : 집 나가기, 도보를 걷기, 마트에 들어가기
- "집 나가기" : 현관가기, 문 열기, 문 잠그기
- "현관 가기" : 오른 발 내딛기, 오른 발 밀기, 왼발 내딛기 ...

위 방법이 Hierarchical 접근법의 본질입니다.

그러나 대부분의 Hierarchical RL 방법론에는 두 가지 문제점이 있다고 저자들은 말합니다.
1. Task-specific 한 design이 필요하기에 Generality가 부족하다.
2. On-policy Learning을 채택하여 시간이 오래걸리고 Sample efficiency가 낮다.

<br>  

## HIRO : HIerarchical Reinforcement learning with Off policy correction
---

저자들은 위 문제점을 개선하기 위해 두 가지를 제시합니다.

- Generality 를 위해 High-level controller 가 자동으로 low-level controller에게 목표를 부여하는 scheme을 디자인했다.
- Efficiency 를 위해 off-policy experience를 활용하는 방법론을 제시했다.

알고리즘은 복잡하여 이해하기 어렵기 때문에 저자가 실험에 사용한 환경과 함께 살펴보겠습니다.

<p align="center">
<img src = "https://user-images.githubusercontent.com/23326843/168213588-c05efe97-93a9-4a5f-be1d-09800dbf51c7.png" width="100%" />
</p>

저자는 Mujoco 시뮬레이션에서 동작하는 4개의 환경을 제시했습니다.  
Ant Gather : 개미의 관절을 움직여서 초록색 공을 먹으면 보상을 받고, 빨간색 공을 먹으면 패널티를 받는다.  
Ant Maze : 개미의 관절을 움직여서 ㄷ자의 미로를 돌아 목표지점으로 간다.
Ant Push : 개미의 관절을 움직여서 빨간 블럭을 밀고 목표 지점으로 간다.  
Ant Fall : 개미의 관절을 움직여서 빨간 블럭을 밀어 틈에 떨어뜨려 다리를 만들고 목표 지점으로 간다.

Ant Gather의 동영상은 [여기](https://sites.google.com/view/efficient-hrl)서 확인할 수 있습니다.

동영상을 염두에 두면서 구체적인 Algorithm을 보겠습니다.

<p align="center">
<img src = "https://user-images.githubusercontent.com/23326843/168236879-0c6f3de0-09e1-4624-935d-812cbcb6c017.png"
 width="100%" />
</p>

HIRO 는 high, low 두 개의 층로 구성되어 있으며, 각 층은 continous version 이라 할 수 있는 DDPG([Lillicrap et al.,2015](https://arxiv.org/abs/1509.02971) 계열 알고리즘 TD3([Fujimoto et al., 2018](https://arxiv.org/abs/1802.09477))를 사용합니다.
따라서 두 층은 각각 $\mu^{lo}$ 와 $\mu^{hi}$ 라는 policy network 를 가지고있습니다.

<br>

학습이 진행되면 $\mu^{hi}$ 는 $\mu^{lo}$ 가 학습해야할 목표인 $g_t$를 임의의 간격 c마다 할당해줍니다. c를 주는 이유는 $\mu^{lo}$ 의 action이 내린 $g_t$에 대한 학습을 충분히 할 수 있을 만큼의 여유를 주기 위해서입니다.

<br>

$\mu^{lo}$ 는 $t$ 마다 $s_t$와 내려진 $g_t$를 고려해 원시적인 액션$a_t$(Ant 환경에서는 관절 각도)를 행합니다. environment 는 그에 맞는 $R_t$를 산출하고 $s_{t+1}$ 로 transition 된다는 것은 여느 강화학습과 동일합니다.

<br>

다만 주의할 점이 있습니다.
$\mu^{hi}$ 와 $\mu^{lo}$ 는 보상함수를 다르게 설계해야 합니다. Ant Gather 환경을 염두에 두고 생각해보겠습니다. $g_t$는 지금 $s_t$ 의 x,y 좌표를 (0,0)으로 뒀을 때 어디로 향해야하는지를 가르쳐주는 (x,y) 좌표입니다. 따라서 $\mu^{lo}$는 $g_t$를 향해 움직이려면 어떻게 관절각도를 조절해야하는지를 학습하고 $\mu^{hi}$ 는 개미의 움직임이 어디로 향해야하는지를 학습하는 구조라고 할 수 있습니다.

<br>

$g_t$ 를 향해 움직이는 방법을 학습하려면 $g_t$에 가까워 지면 보상을 받도록 설계해야합니다. 이를 위해 논문에서는 *intrinsic reward function*$r$ 을 도입했습니다.
$r$은 여러가지로 디자인 할 수 있겠지만 논문에서는 t스텝 이후 내려진 goal쪽으로 실제로 잘 갔는지 거리를 계산($-\lvert\lvert s_t + g_t - s_{t+1}\rvert\rvert_2$) 하여 멀리 떨어질수록 패널티를 줬습니다. 

<br>

c간격동안에는 같은 $g_t$를 내려주지 않고 $h(s_{t-1}, g_{t-1}, s_{t})$ 로 업데이트를 하는 이유도 비슷합니다. 개미를 정확히 북동쪽으로 이동시키려면 $g_t$ 는 (1.0,1.0), (2.0,2.0) 이런 값들을 산출해야합니다. $g_t$값을 업데이트하지 않고 고정해버리면 $g_t$를 향해 잘 가고 있다고 하더라도 $s_t$ 가 계속 바뀌면서 $s_t + g_t$가 점점 멀어집니다.

<br>

이제 $\mu^{hi}$ 의 학습을 생각해봅시다. $\mu^{hi}$ 는 environment 본연의 목적을 푸는 방법을 학습해야합니다. 초록색 공을 모으던지, 미로를 풀던지, 물체를 밀던지 하는 것입니다.
이는 Env 자체에서 산출되는 $R$을 통해 학습할 수 있습니다. 다만 $\mu^{hi}$ 의 action 은 goal $g$ 를 내리는 것이고 이는 time interval c마다 하기 때문에, c간격동안의 Return을 전부 모아야겠죠.

<br>
<br>
<p align="center">
<img src = "https://user-images.githubusercontent.com/23326843/168239906-ef088980-3142-4d28-bd5a-143df8be5790.png"
 width="100%" />
</p>

논문에서는 Sample efficiency를 위해 Off-policy 알고리즘으로 만들기를 원했기 때문에, 각 layer는 trajectory 를 저장하고 리플레이 버퍼에서 샘플링을 하여 Q함수를 업데이트 합니다. 

<br>

$\mu^{lo}$의 업데이트는 간단합니다. input에 우리가 Q러닝을 할 때 넣던 요소에다가 $g_t$ 만 네트워크의 input으로 추가해주면 되죠. reward는 intrinsic reward 를 받구요.
$\mu^{hi}$는 시간간격 c 마다 action $g_t$ 를 하기때문에 trajectory를 c만큼의 stride를 두어 학습하면 됩니다. 그러나 여기서 non-stationary problem 이 발생합니다.
보상 $\sum R_{t:t+c-1}$은 $\mu^{lo}$에 의해 수집됩니다. 그러나 $\mu^{lo}$는 학습동안 계속 업데이트 됩니다. 따라서 업데이트 되기 전 $\mu^{lo}$ 을 통해 받은 리턴 $\sum R_{t:t+c-1}$ 로 $\mu^{hi}$ 를 학습하면 학습이 잘 되지 않는 것이죠.

<br>

물론 $\mu^{lo}$가 안정화될 때까지 많은 학습을 진행하면 결국 $\mu^{hi}$ 도 수렴할 수도 있을겁니다. 그러나 on-policy가 아닌 off-policy를 사용하는 이유는 버리는 데이터 없이 Training 하기 위해서라는 것이 저자들의 목표이기 때문에 off-policy correction 방법을 도입한 것이 아닌가 생각합니다.

<br>


<p align="center">
<img src = "https://user-images.githubusercontent.com/23326843/168244038-03b8bb19-7136-4ee0-9aeb-36c51a7416cd.png"
 width="100%" />
</p>

<br>

Non-stationary problem을 해결하기 위해서 저자들은 많은 시도를 했습니다. 직관적으로 생각했을 때 바꾸는 경우의 수는 세 가지가 있습니다. 
1. 보상을 바꾸는 방법
2. $g_t$를 바꾸는 방법
3. 다음 state $s_{t+1}$ 을 바꾸는 방법

model-based approach를 통해 $g_t$ 와 $s_{t+1}$ 을 바꾸는 방법은 이 포스팅에서는 생략하도록 하겠습니다. 

### 1. 보상을 바꿔보자
보상을 바꾸는 방법은 Sutton 교수님의 저서에 언급된 importance sampling 방법이 가장 먼저 떠오릅니다. 지금 $\mu^{lo}$에서 각 행동을 할 확률에서 update 되기 이전의 $\mu^{lo}$ 에서 각 행동을 할 확률을 나누어 weight로 사용하는 방법입니다.


<p align="center">
<img src = "https://user-images.githubusercontent.com/23326843/168244642-fe433b14-e498-4440-9f23-5bf32c8899ae.png"
 width="70%" />
</p>

간단하고 unbiased 한 좋은 방법이지만, continous domain에서는 action의 확률이 너무 작기때문에 weight의 variance가 너무 커져 실용적이지 않았다고 저자들은 설명합니다.

### 2. $g_t$를 바꿔보자

마찬가지로 importance를 활용해서 $g_t$를 바꿀 수도 있습니다. 만약 어떤 $\~g_t$가 주어졌을 때 지금 $\mu^{lo}$ 에서 산출되는 행동들이 이전 $\mu^{lo}$ 에서 산출되는 행동들과 정확히 같다면 그 $\~g_t$을 $g$ 항에 넣고 학습하면 되니까요. 

<p align="center">
<img src = "https://user-images.githubusercontent.com/23326843/168245126-2113ca66-1f69-4c80-b51c-4171bf65edd5.png"
 width="70%" />
</p>

그러나 이는 biased한 estimator입니다. 액션들이 같을 확률이 1에 가까워진다는 보장이 없죠.

<br>

다음은 저자들이 사용한 주요 방법입니다. 바로 likelihood를 사용하는 건데요.

<p align="center">
<img src = "https://user-images.githubusercontent.com/23326843/168249755-4db4026b-b321-4ad6-85e7-02e5579237a5.png" width="80%" />
</p>

대부분의 RL 알고리즘은 랜덤한 exploration을 해서 action을 선택하므로, 특정 행동이 나올 확률을 정규분포라고 가정합니다. 따라서 $\~g_t$가 주어졌을 때 $\sum R_{t:t+c-1}$ 을 받는 action sequence $a_{t:t+c}$ 가 나올 확률은 $\mu^{lo}(s_i,\~g_i)$를 평균으로 하는 정규분포를 따른다고 간주한 뒤, action sequence $a_{t:t+c}$가 나올 확률이 가장 높은 $\~g_t$ 를 $g$ 항에 집어넣는 겁니다. 그러나 neural network의 input 을 타겟으로 하는 최적화는 그 자체로 Optimization 문제이며, 굉장히 많은 계산이 소요됩니다. 논문에서는 그냥 랜덤하게 $s_{t+c} - s_{t}$ 를 평균으로 하는 normal 분포에서 후보를 뽑아서, 가장 높은값을 부여하는 방식으로 실현했습니다. biased 한 estimator지만 practice에서 잘 작동한다는 점을 강조하고 있네요.


## Result
---

<p align="center">
<img src = "https://user-images.githubusercontent.com/23326843/168254759-73ba81f1-36b4-4eef-9860-68af5e67a47d.png" width="100%" />
</p>

앞서 제시한 4개의 환경을 잘 풀었다는 결론입니다. 이 중 With pre-training 같은 경우는 low 만 20000개를 학습한 뒤에 high를 학습하는 방법입니다. 밑에 비교군은 이전에 있었던 HRL 알고리즘들입니다.

## Conclusion

Generality를 automatical 한 goal assignment로 해결하고, Sample efficiency를 off-policy correction으로 해결했다는 점이 main contribution이 되겠습니다.

