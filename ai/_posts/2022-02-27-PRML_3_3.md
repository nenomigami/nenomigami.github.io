---
layout: post
title: "[ML]베이지안 선형회귀 시각화 (PRML Ch3)"

category: study
sitemap: false
hide_last_modified: true
---

## 베이지안 선형회귀 시각화 (PRML 3.3) 

### 회귀 문제를 풀기 위한 세 가지 방식
<br>
  
1. 결합 밀도 $p(x, t)$ 를 구하는 추론 문제를 풀어낸다. 다음에 이를 정규화하여 조건부 밀도 $p(t|x)$ 를 구하고 최종적으로 식 $\int tp(t|\mathbf{x}) dt$ 의 조건부 평균을 구한다.
2. 조건부 밀도 $p(x|\mathbf{x})$ 를 구하는 추론 문제를 풀고 $\int tp(t|\mathbf{x})dt$ 의 조건부 평균을 구한다.
3. 훈련 데이터로부터 회귀 함수 $y(x)$를 직접 구한다.

=> 이번 문제에서는 3번의 방식으로 직접 구한다

1. 가정

    - 회귀식 : 선형기저함수 모델을 가정  
    $ y(\mathbf{x},\mathbf{w}) = \mathbf{w^T}\phi(\mathbf{x}) $ 여기서 $\mathbf{w}$는 가중치 vector, $\phi$ 는 $\mathbf{x}$를 변환시키는 함수

    - 타깃 변수 : $t = y(\mathbf{x},\mathbf{w}) + \epsilon$,  where $ \epsilon \sim  \mathcal{N}(0, \beta^{-1})$  
    따라서 $ t \sim \mathcal{N(y(\mathbf{x},\mathbf{w}), \beta^{-1})}$  
    즉 t는 가우시안분포로 모델링

    - cost function : Sum of squared Error(SSE)

    - 가중치의 prior 분포는 $\mathbf{w} \sim \mathcal{N}(\mathbf{m_0}, \mathbf{S_0})$ 의 다변량정규분포를 따른다. 여기서 $\mathbf{m_0}$ 은 평균, $\mathbf{S_0}$ 은 공분산  
      편의를 위해 $\mathbf{m_0}$는 0, $\mathbf{S_0}$는 $a^{-1}\mathbf{I}$ 로 가정한다
    - $a = 2.0, \beta = 25$

2. 직선 피팅 예시
   - input은 단일입력변수 $x$, 즉 feature 가 1개.
   - target은 단일타겟변수 $t$
   - $\phi$ 는 identity function
   - 직선식 : $y(x,\mathbf{w}) = w_0 + w_1 x$
   - 모의 데이터 생성식 : $f(x) = -0.3 + 0.5 x + \epsilon$, where $x \sim \mathsf{U}_{[-1,1]}$ , $\epsilon \sim \mathcal{N}(0, \beta^{-1})$, where $   \beta^{-1}=0.2^2$ 

3. $\mathbf{w}$의 베이지안 방식추정<br>
   - likelihood : $p(t|\mathbf{w}) = \prod_{n=1}^N\mathcal{N(\mathbf{w^T}\mathbf{x_n},  \beta^{-1})}$
   - prior : $p(\mathbf{w}) = \mathcal{N(0, a^{-1}\mathbf{I})}$ 
   - posterior : $\mathcal{N}( \mathbf{m}_N, \mathbf{S}_N)$, where   
     $\mathbf{m}_N = \beta\mathbf{S}_N\Phi^T\mathbf{t}$  
     $\mathbf{S}_N^{-1} = \alpha\mathbf{I}+\beta\Phi^T\Phi$
  

### 목표 : 모의 데이터가 하나씩 추가될 때마다 $\mathbf{w}$ 에 해당하는 [-0.3, 0.5]를 추정해나가는 과정을 살펴보는 것   


```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal, norm
```


```python
np.random.seed(1009)
```


```python
a0,a1 = -0.3,0.5
alpha = 2.0
beta = 25
data_x = np.random.uniform(-1, 1, 20)
noise = np.random.normal(0, (1/beta)**(1/2), 20)
data_y = a0 + a1 * data_x + noise
```

시뮬레이션 데이터 포인트 매핑


```python
plt.scatter(data_x, data_y)
plt.xlim(-1,1)
plt.xlabel("x")
plt.ylim(-1,1)
plt.ylabel("y", rotation=0)
plt.show()
```

![BayesianRegression_11_0](https://user-images.githubusercontent.com/23326843/155880947-aa87d6c8-fd2f-4905-9344-05608988c275.png)


```python
w0, w1 = np.mgrid[-1:1:.01, -1:1:.01]
pos = np.dstack((w0, w1))
mean = [0.0, 0.0]
cov = [[(1/alpha), 0], [0, (1/alpha)]]
mvn = multivariate_normal(mean, cov)

w0_point, w1_point = np.random.multivariate_normal(mean, cov, 6).T
x = np.linspace(-1, 1, 100)
y = [w0_point[i] + w1_point[i] * x for i in range(6)]
```

첫번째 열은 가능도함수  
두번째 열은 사후분포(데이터가 없는경우 사전분포)  
세번째 열은 두번째 그래프에서 추출한 w0, w1으로 그린 직선함수와, 데이터포인트를 나타냄

1) 데이터 포인트가 관측되지 않은 경우


```python
fig, axs = plt.subplots(1,3, figsize = (12, 4), constrained_layout=True)
axs[0].set_title("likelihood")

axs[1].set_title("prior")
axs[1].contourf(w0, w1, mvn.pdf(pos))
axs[1].set_xlabel('w0')
axs[1].set_ylabel('w1', rotation=0)

axs[2].set_title("data space")
axs[2].set_xlabel('x')
axs[2].set_ylabel('y', rotation = 0)
for i in range(6):
    axs[2].plot(x,y[i], color = 'r')
axs[2].set_ylim(-1, 1)
axs[2].set_xlim(-1, 1)
plt.show()
```


    
![BayesianRegression_14_0](https://user-images.githubusercontent.com/23326843/155880966-3c98eb49-be3c-402c-bcfe-9d7b8782148b.png)
    


2. 데이터 포인트 1개 관찰


```python
data_x[0], data_y[0] #(x, t)
#1번 열 likelihood 계산
z = np.zeros_like(w0)
w0, w1 = np.mgrid[-1:1:.01, -1:1:.01] # w0 :-1부터 1까지 0.01 간격으로 행 하나 만들고, w1 성분 갯수만큼 broadcast, w1도 동일
pos = np.dstack((w0, w1)) # shape이 같은 매트릭스를 새로운 axis 로 합친다
for i in range(len(w0[:,0])):
    for j in range(len(w1[0,:])):
        likelihood_mean = w0[:,0][i] + w1[0,:][j] * data_x[0]
        likelihood_std = (1/beta)**(1/2)
        z[i,j] = norm(loc=likelihood_mean, scale = likelihood_std).pdf(data_y[0])

#2번 열 posterior 계산
phi = np.array([[1, data_x[0]]])# 1x2 shape
posterior_cov_inv = alpha * np.array([[1,0],[0,1]]) + beta * np.matmul(phi.T, phi)
posterior_cov = np.linalg.inv(posterior_cov_inv)
posterior_mean = beta * np.matmul(posterior_cov, np.matmul(phi.T,np.array(data_y[0]).reshape(1,1)))
mvn = multivariate_normal(posterior_mean.ravel(), posterior_cov)

#3번 열 datapoint 표시
w0_point, w1_point = np.random.multivariate_normal(posterior_mean.ravel(), posterior_cov, 6).T
x = np.linspace(-1, 1, 100)
y = [w0_point[i] + w1_point[i] * x for i in range(6)]
```


```python
fig, axs = plt.subplots(1,3, figsize = (12, 4), constrained_layout=True)
axs[0].set_title("likelihood")
axs[0].contourf(w0,w1,z)
axs[0].set_xlabel('w0')
axs[0].set_ylabel('w1', rotation=0)
axs[0].scatter(a0, a1,color="red",marker="+")

axs[1].set_title("posterior")
axs[1].contourf(w0, w1, mvn.pdf(pos))
axs[1].set_xlabel('w0')
axs[1].set_ylabel('w1', rotation=0)

axs[2].set_title("data space")
axs[2].set_xlabel('x')
axs[2].set_ylabel('y', rotation = 0)
for i in range(6):
    axs[2].plot(x,y[i], color = 'r')
axs[2].scatter(data_x[0], data_y[0])
axs[2].set_ylim(-1, 1)
axs[2].set_xlim(-1, 1)
```
    

![BayesianRegression_17_1](https://user-images.githubusercontent.com/23326843/155880978-0558efd9-5c78-4b0a-b20c-d51fa093e6f5.png)


3. 데이터 포인트 2개 관찰


```python
for i,j in zip(data_x[0:2], data_y[0:2]):
    print([i,j])
```

    [0.2297605591287708, -0.45972024745574697]
    [-0.6708507495131852, -0.41819084135729445]
    


```python
#1번 열 likelihood 계산
data_x[0:2], data_y[0:2]

z = np.ones_like(w0)
w0, w1 = np.mgrid[-1:1:.01, -1:1:.01] # w0 :-1부터 1까지 0.01 간격으로 행 하나 만들고, w1 성분 갯수만큼 broadcast, w1도 동일
pos = np.dstack((w0, w1)) # shape이 같은 매트릭스를 새로운 axis 로 합친다
for x,y in zip(data_x[0:2], data_y[0:2]):
    for i in range(len(w0[:,0])):
        for j in range(len(w1[0,:])):
            likelihood_mean = w0[:,0][i] + w1[0,:][j] * x
            likelihood_std = (1/beta)**(1/2)
            z[i,j] *= norm(loc=likelihood_mean, scale = likelihood_std).pdf(y)
```


```python
#1번 열 likelihood 계산
data_x[0:2], data_y[0:2]

z = np.ones_like(w0)
w0, w1 = np.mgrid[-1:1:.01, -1:1:.01] # w0 :-1부터 1까지 0.01 간격으로 행 하나 만들고, w1 성분 갯수만큼 broadcast, w1도 동일
pos = np.dstack((w0, w1)) # shape이 같은 매트릭스를 새로운 axis 로 합친다
for x,y in zip(data_x[0:2], data_y[0:2]):
    for i in range(len(w0[:,0])):
        for j in range(len(w1[0,:])):
            likelihood_mean = w0[:,0][i] + w1[0,:][j] * x
            likelihood_std = (1/beta)**(1/2)
            z[i,j] *= norm(loc=likelihood_mean, scale = likelihood_std).pdf(y)

#2번 열 posterior 계산            
phi = np.concatenate([np.ones_like(data_x[0:2].reshape(2,1)), data_x[0:2].reshape(2,1)],axis=1) # 2x2 shape
posterior_cov_inv = alpha * np.array([[1,0],[0,1]]) + beta * np.matmul(phi.T, phi)
posterior_cov = np.linalg.inv(posterior_cov_inv)
posterior_mean = beta * np.matmul(posterior_cov, np.matmul(phi.T,np.array(data_y[0:2]).reshape(2,1)))
mvn = multivariate_normal(posterior_mean.ravel(), posterior_cov)

#3번 열 datapoint 표시
w0_point, w1_point = np.random.multivariate_normal(posterior_mean.ravel(), posterior_cov, 6).T
x = np.linspace(-1, 1, 100)
y = [w0_point[i] + w1_point[i] * x for i in range(6)]
```


```python
fig, axs = plt.subplots(1,3, figsize = (12, 4), constrained_layout=True)
axs[0].set_title("likelihood")
axs[0].contourf(w0,w1,z)
axs[0].set_xlabel('w0')
axs[0].set_ylabel('w1', rotation=0)
axs[0].scatter(a0, a1,color="red",marker="+")

axs[1].set_title("posterior")
axs[1].contourf(w0, w1, mvn.pdf(pos))
axs[1].set_xlabel('w0')
axs[1].set_ylabel('w1', rotation=0)
axs[1].scatter(a0, a1,color="red",marker="+")

axs[2].set_title("data space")
axs[2].set_xlabel('x')
axs[2].set_ylabel('y', rotation = 0)
for i in range(6):
    axs[2].plot(x,y[i], color = 'r')
axs[2].scatter(data_x[0:2], data_y[0:2])
axs[2].set_ylim(-1, 1)
axs[2].set_xlim(-1, 1)
```



![BayesianRegression_22_1](https://user-images.githubusercontent.com/23326843/155880998-3be5a37b-b437-4cdb-ba06-2022830cae37.png)


4. 데이터 포인트 20개 관찰


```python
data_x, data_y

#1번 열 likelihood 계산
z = np.ones_like(w0)
w0, w1 = np.mgrid[-1:1:.01, -1:1:.01] # w0 :-1부터 1까지 0.01 간격으로 행 하나 만들고, w1 성분 갯수만큼 broadcast, w1도 동일
pos = np.dstack((w0, w1)) # shape이 같은 매트릭스를 새로운 axis 로 합친다
for x,y in zip(data_x, data_y):
    for i in range(len(w0[:,0])):
        for j in range(len(w1[0,:])):
            likelihood_mean = w0[:,0][i] + w1[0,:][j] * x
            likelihood_std = (1/beta)**(1/2)
            z[i,j] *= norm(loc=likelihood_mean, scale = likelihood_std).pdf(y)

#2번 열 posterior 계산            
phi = np.concatenate([np.ones_like(data_x.reshape(20,1)), data_x.reshape(20,1)],axis=1) # 20x2 shape
posterior_cov_inv = alpha * np.array([[1,0],[0,1]]) + beta * np.matmul(phi.T, phi)
posterior_cov = np.linalg.inv(posterior_cov_inv)
posterior_mean = beta * np.matmul(posterior_cov, np.matmul(phi.T,np.array(data_y).reshape(20,1)))
mvn = multivariate_normal(posterior_mean.ravel(), posterior_cov)

#3번 열 datapoint 표시
w0_point, w1_point = np.random.multivariate_normal(posterior_mean.ravel(), posterior_cov, 6).T
x = np.linspace(-1, 1, 100)
y = [w0_point[i] + w1_point[i] * x for i in range(6)]
```


```python
fig, axs = plt.subplots(1,3, figsize = (12, 4), constrained_layout=True)
axs[0].set_title("likelihood")
axs[0].contourf(w0,w1,z)
axs[0].set_xlabel('w0')
axs[0].set_ylabel('w1', rotation=0)
axs[0].scatter(a0, a1,color="red",marker="+")

axs[1].set_title("posterior")
axs[1].contourf(w0, w1, mvn.pdf(pos))
axs[1].set_xlabel('w0')
axs[1].set_ylabel('w1', rotation=0)
axs[1].scatter(a0, a1,color="red",marker="+")

axs[2].set_title("data space")
axs[2].set_xlabel('x')
axs[2].set_ylabel('y', rotation = 0)
for i in range(6):
    axs[2].plot(x,y[i], color = 'r')
axs[2].scatter(data_x, data_y)
axs[2].set_ylim(-1, 1)
axs[2].set_xlim(-1, 1)
```

![BayesianRegression_25_1](https://user-images.githubusercontent.com/23326843/155881009-aa2aabe3-397d-49ee-9a43-76745190ae39.png)

